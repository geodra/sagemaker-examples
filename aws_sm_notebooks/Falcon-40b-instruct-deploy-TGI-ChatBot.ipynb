{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4c1328-037f-4085-a3b9-e6a3352bdbc5",
   "metadata": {},
   "source": [
    "# 1.Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b6a052-2c26-4289-841f-9b99fd55e374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.163.0\" --upgrade --quiet\n",
    "!pip install transformers langchain --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30cc32-c14f-4ed0-8b13-388d816595b4",
   "metadata": {},
   "source": [
    "# 2. Deploy Falcon 40B model on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786bce3b-7267-48aa-ad10-c2c58f4e7eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::069230569860:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker session region: eu-west-1\n",
      "llm image uri: 763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n",
      "falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "hf_model_id = \"tiiuae/falcon-40b-instruct\" # model id from huggingface.co/models\n",
    "instance_type = \"ml.g5.12xlarge\" # instance type to use for deployment\n",
    "number_of_gpu = 4 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 300 # Increase the timeout for the health check to 5 minutes for downloading the model\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env={\n",
    "    'HF_MODEL_ID': hf_model_id,\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    # 'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "  }\n",
    ")\n",
    "\n",
    "model_name = hf_model_id.split(\"/\")[-1].replace(\".\", \"-\")\n",
    "endpoint_name = name_from_base(model_name)\n",
    "print(endpoint_name)\n",
    "\n",
    "# llm = llm_model.deploy(\n",
    "#   initial_instance_count=1,\n",
    "#   instance_type=instance_type,\n",
    "#   container_startup_health_check_timeout=health_check_timeout,\n",
    "#   endpoint_name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6ddd-ad28-49ff-8e73-d53968dc6041",
   "metadata": {},
   "source": [
    "# 3. Invoke model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "494e2a7c-a69f-4e38-b195-d4b2d8f3f14b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sun on my face and the sand between my toes\n",
      "Walking the streets of Valencia, feeling free and alive\n",
      "The salty ocean air carries me to new heights\n",
      "As the city comes alive with the sounds of the street\n",
      "The charming buildings of the old city stand tall\n",
      "As the modern city skyline shines in the distance\n",
      "The history and culture of Valencia are all around me\n",
      "As the city embraces me with open arms\n",
      "The food and the music are a delight to my senses\n",
      "As I experience the Valencia lifestyle to its fullest\n",
      "The colorful architecture and vibrant atmosphere\n",
      "Make me feel like I never want to leave this beautiful place.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# grab environment variables\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "message = \"Write a poem about Valencia\"\n",
    "prompt = f\"{message}\"\n",
    "parameters = {\n",
    "    \"best_of\": None, \n",
    "    \"temperature\": .7,\n",
    "    \"repetition_penalty\": None,\n",
    "    \"top_k\": None,\n",
    "    \"top_p\": None,\n",
    "    \"typical_p\": None,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 200,  # tmax number of tokens return; keep in mind prompt +output has to be less than 2048 tokens; be frugal\n",
    "    \"return_full_text\": False, # to not return the prompt as part of the ouput\n",
    "    \"stop\": [],\n",
    "    \"truncate\": None,\n",
    "    \"watermark\": True,\n",
    "    \"details\": False,\n",
    "    \"seed\": 5,\n",
    "  }\n",
    "input_data = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {**parameters}\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(input_data).encode('utf-8'))\n",
    "response_json = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "\n",
    "print(response_json[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c638b",
   "metadata": {},
   "source": [
    "# 4. Create a LangChain ChatBot With Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d963870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    model_kwargs = {}\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        self.model_kwargs = model_kwargs\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        text = response_json[0][\"generated_text\"].strip()\n",
    "        if \"stop\" in self.model_kwargs:\n",
    "            for stop in self.model_kwargs[\"stop\"]:\n",
    "                text = text.replace(stop,\"\")\n",
    "        return text\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b786700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of 2021, the estimated population of Greece is approximately 10.7 million people.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_llm(prompt=\"What is the population of Greece?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "563296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "def print_ww(*args, **kwargs):\n",
    "    buffer = StringIO()\n",
    "    try: \n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        width = 100\n",
    "        if 'width' in kwargs:\n",
    "            width = kwargs['width']\n",
    "            del kwargs['width']\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()    \n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4a6ec349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "User: Hi there!\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import (ConversationBufferMemory, \n",
    "                              ConversationSummaryMemory, \n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationKGMemory)\n",
    "# from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "#                                                   ConversationSummaryMemory, \n",
    "#                                                   ConversationBufferWindowMemory,\n",
    "#                                                   ConversationKGMemory)\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "params = {\n",
    "    \"temperature\": .8,\n",
    "    \"top_p\": .9,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 1024,  # tmax number of tokens return; keep in mind prompt +output has to be less than 2048 tokens; be frugal\n",
    "    \"return_full_text\": False, # to not return the prompt as part of the ouput\n",
    "    \"stop\": [ \"User:\", \"<|endoftext|>\"],\n",
    "    \"seed\": 5,\n",
    "    \"stop1\": ['bb'],\n",
    "  }\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=params,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "human_prefix = \"User\"\n",
    "ai_prefix = \"Falcon\"\n",
    "# turn verbose to true to see the full logs and documents\n",
    "conversation = ConversationChain(\n",
    "    llm=sm_llm, verbose=True, memory=ConversationBufferMemory(human_prefix = human_prefix,\n",
    "                                                              ai_prefix = ai_prefix) #memory_chain\n",
    ")\n",
    "\n",
    "# langchain prompts do not always work with all the models. This prompt is tuned for Claude\n",
    "flacon_prompt = PromptTemplate.from_template(\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
    "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
    "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
    "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
    "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
    "the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "User: {input}\n",
    "\n",
    "Falcon:\n",
    "\"\"\")\n",
    "                                             \n",
    "conversation.prompt = flacon_prompt\n",
    "\n",
    "print_ww(conversation.predict(input=\"Hi there!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5873561",
   "metadata": {},
   "source": [
    "### New Questions\n",
    "\n",
    "Model has responded with intial message, let's ask few questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac30d440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Falcon: Hello!\n",
      "\n",
      "\n",
      "\n",
      "User: Give me a few tips on how to start a new garden.\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To start a new garden, you can follow these steps:\n",
      "1. Choose a location with adequate sunlight and fertile soil.\n",
      "2. Determine what you want to grow in your garden.\n",
      "3. Prepare the soil by removing any weeds and adding compost.\n",
      "4. Plant your seeds or seedlings according to the instructions on the packet.\n",
      "5. Water your plants regularly and fertilize them as needed.\n",
      "6. Harvest your crops when they are ready and enjoy your homegrown produce!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"Give me a few tips on how to start a new garden.\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d680b0",
   "metadata": {},
   "source": [
    "### Build on the questions\n",
    "\n",
    "Let's ask a question without mentioning the word garden to see if model can understand previous conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6ee085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Falcon: Hello!\n",
      "\n",
      "\n",
      "User: Give me a few tips on how to start a new garden.\n",
      "Falcon: To start a new garden, you can follow these steps:\n",
      "1. Choose a location with adequate sunlight and fertile soil.\n",
      "2. Determine what you want to grow in your garden.\n",
      "3. Prepare the soil by removing any weeds and adding compost.\n",
      "4. Plant your seeds or seedlings according to the instructions on the packet.\n",
      "5. Water your plants regularly and fertilize them as needed.\n",
      "6. Harvest your crops when they are ready and enjoy your homegrown produce!\n",
      "\n",
      "User: Cool. Will that work with tomatoes?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, it will work with tomatoes. Tomatoes are a popular choice for home gardens and can be grown in\n",
      "a variety of environments. Just be sure to provide them with plenty of sunlight and fertile soil.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"Cool. Will that work with tomatoes?\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d240d13",
   "metadata": {},
   "source": [
    "### Finishing this conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb451a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: Hi there!\n",
      "Falcon: Hello!\n",
      "\n",
      "\n",
      "User: Give me a few tips on how to start a new garden.\n",
      "Falcon: To start a new garden, you can follow these steps:\n",
      "1. Choose a location with adequate sunlight and fertile soil.\n",
      "2. Determine what you want to grow in your garden.\n",
      "3. Prepare the soil by removing any weeds and adding compost.\n",
      "4. Plant your seeds or seedlings according to the instructions on the packet.\n",
      "5. Water your plants regularly and fertilize them as needed.\n",
      "6. Harvest your crops when they are ready and enjoy your homegrown produce!\n",
      "User: Cool. Will that work with tomatoes?\n",
      "Falcon: Yes, it will work with tomatoes. Tomatoes are a popular choice for home gardens and can be grown in a variety of environments. Just be sure to provide them with plenty of sunlight and fertile soil.\n",
      "\n",
      "User: That's all, thank you!\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You're welcome! Is there anything else I can help you with?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"That's all, thank you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371dac5",
   "metadata": {},
   "source": [
    "### Interactive session using ipywidgets\n",
    "The following utility class allows us to interact with Claude in a more natural way. We write out question in an input box, and get Claude answer. We can then continue our conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef9650d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.run({'question': prompt })\n",
    "                    else:\n",
    "                        result = self.qa.run({'input': prompt }) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print_ww(f\"Falcon:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "            \n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fae7b2",
   "metadata": {},
   "source": [
    "Let's start a chat. You can also test the following questions:\n",
    "tell me a joketell me another jokewhat was the first joke aboutcan you make another joke on the same topic of the first joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e1f7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory = ConversationBufferMemory(human_prefix = human_prefix,\n",
    "                                                ai_prefix = ai_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "327ac4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6609a64a78e948cf946f7c9bd1836fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(conversation)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcea1b",
   "metadata": {},
   "source": [
    "### Chatbot with persona\n",
    "\n",
    " AI assistant will play the role of a career coach. Role Play Dialogue requires user message to be set in before starting the chat. ConversationBufferMemory is used to pre-populate the dialog # store previous interactions using ConversationalBufferMemory and add custom prompts to the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52b85fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: You will be acting as a career coach. Your goal is to give career advice to users\n",
      "Falcon: I am career coach and give career advice\n",
      "\n",
      "User: What are the career options in AI?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The career options in AI are numerous and varied, including roles such as AI language model, data\n",
      "scientist, machine learning engineer, and more. Some examples of specific jobs within these fields\n",
      "include natural language processing engineer, computer vision engineer, and AI language model\n",
      "trainer. There are also opportunities for roles that involve working with AI in fields such as\n",
      "healthcare, finance, and education.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(human_prefix = human_prefix,\n",
    "                                ai_prefix = ai_prefix)\n",
    "memory.chat_memory.add_user_message(\"You will be acting as a career coach. Your goal is to give career advice to users\")\n",
    "memory.chat_memory.add_ai_message(\"I am career coach and give career advice\")\n",
    "conversation.memory = memory \n",
    "\n",
    "print_ww(conversation.predict(input=\"What are the career options in AI?\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5dc7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: You will be acting as a career coach. Your goal is to give career advice to users\n",
      "Falcon: I am career coach and give career advice\n",
      "User: What are the career options in AI?\n",
      "Falcon: The career options in AI are numerous and varied, including roles such as AI language model, data scientist, machine learning engineer, and more. Some examples of specific jobs within these fields include natural language processing engineer, computer vision engineer, and AI language model trainer. There are also opportunities for roles that involve working with AI in fields such as healthcare, finance, and education.\n",
      "\n",
      "User: What these people really do? Is it fun?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The work that these people do is incredibly important and fulfilling. They work on developing AI\n",
      "systems that can solve real-world problems and make a positive impact on society. The work can be\n",
      "challenging, but it is also incredibly rewarding. As for whether it is fun, that depends on the\n",
      "individual. Some people may find it exhilarating to work on cutting-edge technology and solve\n",
      "complex problems, while others may find it more enjoyable to work in a more traditional field.\n",
      "Ultimately, the best way to determine if a career in AI is right for you is to try it out and see\n",
      "for yourself.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"What these people really do? Is it fun?\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c1f11",
   "metadata": {},
   "source": [
    "Let's ask a question that is not specialty of this Persona and the model shouldn't answer that question and give a reason for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aaf913bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I am not capable of fixing cars. However, there are plenty of resources\n",
      "available online or in books that can help you troubleshoot and fix common car problems. You can\n",
      "also take your car to a mechanic who specializes in auto repair.\n"
     ]
    }
   ],
   "source": [
    "conversation.verbose = False\n",
    "print_ww(conversation.predict(input=\"How to fix my car?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa902f",
   "metadata": {},
   "source": [
    "### Memory type #2: ConversationSummaryMemory\n",
    "The problem with the `ConversationBufferMemory` is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed.\n",
    "\n",
    "Enter ConversationSummaryMemory.\n",
    "\n",
    "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue.\n",
    "\n",
    "Key feature: the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM.\n",
    "\n",
    "In this case we need to send the llm to our memory constructor to power its summarization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01fcaa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "human_prefix = \"User\"\n",
    "ai_prefix = \"Falcon\"\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = sm_llm,\n",
    "                                human_prefix = human_prefix,\n",
    "                                ai_prefix = ai_prefix)\n",
    "print(memory.prompt.template)\n",
    "\n",
    "flacon_sum_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
    "\n",
    "\n",
    "Current summary:\n",
    "The user asks what Flacon thinks of artificial intelligence. Falcon thinks artificial intelligence is a force for good.\n",
    "\n",
    "New lines of conversation:\n",
    "User: Why do you think artificial intelligence is a force for good?\n",
    "Falcon: Because artificial intelligence will help humans reach their full potential.\n",
    "\n",
    "New summary:\n",
    "The user asks what Falcon thinks of artificial intelligence. Falcon thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\"\"\")\n",
    "\n",
    "memory.prompt = flacon_sum_prompt\n",
    "\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "conversation= ConversationChain(\n",
    "    llm=sm_llm, verbose=True, memory=memory #memory_chain\n",
    ")\n",
    "\n",
    "# langchain prompts do not always work with all the models. This prompt is tuned for Claude\n",
    "flacon_prompt = PromptTemplate.from_template(\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
    "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
    "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
    "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
    "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
    "the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "User: {input}\n",
    "\n",
    "Falcon:\n",
    "\"\"\")\n",
    "conversation.prompt = flacon_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bda03",
   "metadata": {},
   "source": [
    "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e21f2f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "User: Good morning!\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Good morning!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"Good morning!\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ee78e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The user greets Falcon. Falcon greets the user.\n",
      "\n",
      "This approach can be continued indefinitely to summarize the conversation and extract important information.\n",
      "\n",
      "User: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What do you mean by integrating Large Language Models with external knowledge?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The user asks what their interest is in integrating Large Language Models with external knowledge. Falcon asks for clarification.\n",
      "\n",
      "This approach can be combined with sentiment analysis to identify key points of interest or opinions in the conversation.\n",
      "\n",
      "User: I just want to analyze the different possibilities. What can you think of?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- The integration of Large Language Models with external knowledge can enhance the overall\n",
      "performance of a system, allowing it to access a wider range of information and provide more\n",
      "accurate responses to user queries.\n",
      "- The integration can also help improve the user experience by providing more personalized and\n",
      "relevant responses to their queries.\n",
      "- Additionally, integrating Large Language Models with external knowledge can lead to more efficient\n",
      "and effective workflows, as well as more efficient use of resources.\n",
      "- However, there are also potential challenges associated with integrating Large Language Models\n",
      "with external knowledge, such as ensuring data privacy and security, ensuring that the integrated\n",
      "systems do not introduce biases or inaccuracies, and ensuring that the integrated systems do not\n",
      "negatively impact user experience or workflows.\n",
      "- Overall, integrating Large Language Models with external knowledge can be a valuable approach, but\n",
      "it requires careful planning, implementation, and ongoing monitoring to ensure optimal results.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The user asks what their interest is in integrating Large Language Models with external knowledge. Falcon provides some possibilities of how the integration can enhance the overall performance of a system, provide more accurate responses, and improve user experience. Falcon also discusses potential challenges associated with integrating Large Language Models with external knowledge.\n",
      "\n",
      "User: Which data source types could be used to give context to the model?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- Documents\n",
      "- Web pages\n",
      "- APIs\n",
      "- Databases\n",
      "- Social media posts\n",
      "- News articles\n",
      "- Video and audio transcripts\n",
      "- Sensor data\n",
      "- Government data sources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The user asks what their interest is in integrating Large Language Models with external knowledge. Falcon provides some possibilities of how the integration can enhance the overall performance of a system, provide more accurate responses, and improve user experience. Falcon also discusses potential challenges associated with integrating Large Language Models with external knowledge. The user asks for specific data source types that could be used to give context to the model. Falcon responds with a list of data source types.\n",
      "\n",
      "New lines of conversation:\n",
      "\n",
      "\n",
      "User: What is my aim again?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The user's aim is to integrate Large Language Models with external knowledge to improve the overall\n",
      "performance of a system, provide more accurate responses, and enhance the user experience.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\"My interest here is to explore the potential of integrating Large Language Models with external knowledge\",\n",
    "               \"I just want to analyze the different possibilities. What can you think of?\",\n",
    "               \"Which data source types could be used to give context to the model?\",\n",
    "               \"What is my aim again?\"\n",
    "              ]:\n",
    "    print_ww(conversation.predict(input=prompt)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1646e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user asks what their aim is again. The aim is to integrate Large Language Models with external knowledge to improve the overall performance of a system, provide more accurate responses, and enhance the user experience.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46853f1",
   "metadata": {},
   "source": [
    "**Note:** Potentially is better to use a dedicated model that is tuned to summarize conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d942e2a",
   "metadata": {},
   "source": [
    "### Memory type #3: ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd601f",
   "metadata": {},
   "source": [
    "Another great option for these cases is the ConversationBufferWindowMemory where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count and the per-call token count will drop noticeably. We will control this window with the k parameter.\n",
    "\n",
    "**Key feature:** the conversation buffer window memory keeps the latest pieces of the conversation in raw form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec0fdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory=ConversationBufferWindowMemory(k=1,\n",
    "                                                   human_prefix = human_prefix,\n",
    "                                                    ai_prefix = ai_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4dd8bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "User: Good morning!\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Good morning!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: Good morning!\n",
      "Falcon: Good morning!\n",
      "\n",
      "\n",
      "\n",
      "User: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What kind of knowledge are you interested in integrating with Large Language Models?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "Falcon: What kind of knowledge are you interested in integrating with Large Language Models?\n",
      "\n",
      "\n",
      "\n",
      "User: I just want to analyze the different possibilities. What can you think of?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Some potential possibilities for integrating Large Language Models with external knowledge include:\n",
      "- Integrating with knowledge bases such as Wikipedia or OpenAI's GPT-3 to provide accurate and up-\n",
      "to-date information.\n",
      "- Combining with domain-specific knowledge to enhance the model's ability to understand and generate\n",
      "text in a particular field.\n",
      "- Collaborating with other AI systems to create a more comprehensive and intelligent system.\n",
      "- Utilizing external data sources such as weather, stock market, and news feeds to inform the\n",
      "model's responses.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: I just want to analyze the different possibilities. What can you think of?\n",
      "Falcon: Some potential possibilities for integrating Large Language Models with external knowledge include:\n",
      "- Integrating with knowledge bases such as Wikipedia or OpenAI's GPT-3 to provide accurate and up-to-date information.\n",
      "- Combining with domain-specific knowledge to enhance the model's ability to understand and generate text in a particular field.\n",
      "- Collaborating with other AI systems to create a more comprehensive and intelligent system.\n",
      "- Utilizing external data sources such as weather, stock market, and news feeds to inform the model's responses.\n",
      "\n",
      "\n",
      "\n",
      "User: Which data source types could be used to give context to the model?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- Time-based data sources such as weather, stock market, and news feeds.\n",
      "- Geographical data sources such as location data, maps, and GPS coordinates.\n",
      "- Social media data sources such as Twitter, Facebook, and Instagram.\n",
      "- User-generated data such as customer reviews, surveys, and feedback.\n",
      "- Industry-specific data sources such as healthcare, finance, and education.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, \n",
      "and a human user, called User. In the following interactions, User and Falcon will converse in natural language, \n",
      "and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. \n",
      "Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied \n",
      "with. It knows a lot, and always tells the truth. If Falcon does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "User: Which data source types could be used to give context to the model?\n",
      "Falcon: - Time-based data sources such as weather, stock market, and news feeds.\n",
      "- Geographical data sources such as location data, maps, and GPS coordinates.\n",
      "- Social media data sources such as Twitter, Facebook, and Instagram.\n",
      "- User-generated data such as customer reviews, surveys, and feedback.\n",
      "- Industry-specific data sources such as healthcare, finance, and education.\n",
      "\n",
      "\n",
      "\n",
      "User: What is my aim again?\n",
      "\n",
      "Falcon:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- To provide a list of data source types that could be used to give context to a model.\n",
      "- To help the user understand the potential sources of information that could be used to train a\n",
      "model.\n",
      "- To answer the user's questions to the best of my ability.\n",
      "- To assist the user in developing their understanding of machine learning.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\"Good morning!\",\n",
    "               \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\",\n",
    "               \"I just want to analyze the different possibilities. What can you think of?\",\n",
    "               \"Which data source types could be used to give context to the model?\",\n",
    "               \"What is my aim again?\"\n",
    "]:\n",
    "    print_ww(conversation.predict(input=prompt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1754e",
   "metadata": {},
   "source": [
    "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be 1, we would expect it remembers only the last interaction.\n",
    "\n",
    "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4e5b27ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is my aim again?\n",
      "Falcon: - To provide a list of data source types that could be used to give context to a model.\n",
      "- To help the user understand the potential sources of information that could be used to train a model.\n",
      "- To answer the user's questions to the best of my ability.\n",
      "- To assist the user in developing their understanding of machine learning.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bufw_history = conversation.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']\n",
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7f95c-c378-4a64-b39d-2868de322a47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Keep in mind the special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e1461f69-a9e4-4c8d-9765-ec2d27687074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>>TITLE<<',\n",
       " '>>ABSTRACT<<',\n",
       " '>>INTRODUCTION<<',\n",
       " '>>SUMMARY<<',\n",
       " '>>COMMENT<<',\n",
       " '>>ANSWER<<',\n",
       " '>>QUESTION<<',\n",
       " '>>DOMAIN<<',\n",
       " '>>PREFIX<<',\n",
       " '>>SUFFIX<<',\n",
       " '>>MIDDLE<<',\n",
       " '<|endoftext|>',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/special_tokens_map.json\n",
    "# https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/tokenizer_config.json\n",
    "\n",
    "# print first 20 tokens\n",
    "tokenizer.convert_ids_to_tokens(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d2629c6-9e75-4980-8c2f-9a9a98026299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>>TITLE<<': 0, '>>ABSTRACT<<': 1, '>>INTRODUCTION<<': 2, '>>SUMMARY<<': 3, '>>COMMENT<<': 4, '>>ANSWER<<': 5, '>>QUESTION<<': 6, '>>DOMAIN<<': 7, '>>PREFIX<<': 8, '>>SUFFIX<<': 9, '>>MIDDLE<<': 10, '<|endoftext|>': 11, '!': 12, '\"': 13, '#': 14, '$': 15, '%': 16, '&': 17, \"'\": 18, '(': 19}\n"
     ]
    }
   ],
   "source": [
    "sorted_dict = dict(sorted(tokenizer.get_vocab().items(), key=lambda x: x[1]))\n",
    "first_20_elements = dict(list(sorted_dict.items())[:20])\n",
    "\n",
    "print(first_20_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435fbf7-90ca-4357-bc84-54486f08cdb7",
   "metadata": {},
   "source": [
    "# Delete all models, endpoint_configs & endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ed1cf1a-f39a-49d1-9715-c23b6e8c5d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: huggingface-pytorch-tgi-inference-2023-07-04-12-09-14-870\n",
      "Deleting endpoint: falcon-40b-instruct\n",
      "Deleting endpoint_config: falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def delete_resources(resource_type):\n",
    "    client = boto3.client('sagemaker')\n",
    "    list_method = getattr(client, f\"list_{resource_type}s\")\n",
    "    delete_method = getattr(client, f\"delete_{resource_type}\")\n",
    "    resource_type_name = resource_type.replace('_', ' ').title().replace(' ', '')\n",
    "    resources = list_method()[f\"{resource_type_name}s\"]\n",
    "    for resource in resources:\n",
    "        resource_name = resource[f\"{resource_type_name}Name\"]\n",
    "        print(f\"Deleting {resource_type}: {resource_name}\")\n",
    "        # if resource_name == \"falcon-40b-instruct\": continue\n",
    "        # if resource_name == \"llama-30b-supercot-2023-06-15-16-54-10-187-endpoint\": continue\n",
    "        delete_method(**{f\"{resource_type_name}Name\": resource_name})\n",
    "\n",
    "def main():\n",
    "    resource_types = ['model', 'endpoint', 'endpoint_config']  # Add more resource types if needed\n",
    "\n",
    "    for resource_type in resource_types:\n",
    "        delete_resources(resource_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950565e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
