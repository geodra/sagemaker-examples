{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fce004-0da0-4337-becb-a7ff859502ca",
   "metadata": {},
   "source": [
    "#  Serve Salesforce/xgen-7b-8k-base model with Amazon SageMaker Hosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b832e6-641a-443c-b533-2396e19b01bc",
   "metadata": {},
   "source": [
    "👋 Hey there! 🌟 \n",
    "\n",
    "Let's take a walk-through together on how to deploy and perform inference on the **Salesforce Xgen-7B-8K-base** model using the **Large Model Inference (LMI)** container provided by AWS with the help of **DJL Serving**. 😄\n",
    "\n",
    "Since the **Salesforce Xgen-7B-8K-base** is a relatively small language model (LLM) that can be easily accommodated on a single GPU, we'll make use of the `ml.g5.2xlarge` instance, which comes with **1** GPU. 🖥️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad5b38-cb9c-4df9-ada8-84d0bed0c91a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbea2a-b6c8-4106-ae69-17c877a00f88",
   "metadata": {},
   "source": [
    "To get started, you'll need to install the necessary dependencies for packaging your model and running inferences on Amazon SageMaker. Don't worry, it's a simple process! Just make sure to update SageMaker and boto3 too. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd50d8cb-1e06-480a-afb9-a5cc770e3c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80a142-7f21-48ca-9a44-1462cd82004c",
   "metadata": {},
   "source": [
    "## Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b43806-e267-4259-8dd8-617d4feb839d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import jinja2\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2cb0e7-e646-4a8b-b9b5-f25c08ec6b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "hf_model_id = 'Salesforce/xgen-7b-8k-base'\n",
    "model_id = hf_model_id.replace('/','-')\n",
    "s3_code_prefix_accelerate = f\"hf-large-model/{model_id}/accelerate\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bdc38-8552-4d2a-b1fe-aeacc229b85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Create SageMaker compatible model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f3f1f-d82e-403c-b966-a1a495a26d5d",
   "metadata": {},
   "source": [
    "To get our model ready for deployment on a SageMaker Endpoint, we need to prepare a few things for both SageMaker and our container. No worries, it's a straightforward process! We'll use a local folder to store these files, including some important ones like serving.properties (which defines parameters for the LMI container) and requirements.txt (to specify the dependencies we need to install). 📂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac8d9604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory_name = f\"code_{model_id.replace('-','_')}_accelerate\"\n",
    "os.makedirs(directory_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c1b6f-213b-4540-b65c-c2c2e029e283",
   "metadata": {},
   "source": [
    "In the serving.properties file, you'll need to define the engine to use and the model you want to host. Pay attention to the tensor_parallel_degree parameter, as it's essential in this scenario. If a single GPU doesn't have enough memory to handle the entire model, you can use tensor parallelism >1 to divide the model into multiple parts.\n",
    "\n",
    "For your deployment, we'll be using a 'ml.g5.2xlarge' instance, which comes with 1 GPU and is sufficient for loading our model. Just make sure not to specify a value larger than what the instance provides, or your deployment might encounter issues. ❌🙅‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212382fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_Salesforce_xgen_7b_8k_base_accelerate/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/serving.properties\n",
    "engine=Python\n",
    "option.model_id={{hf_model_id}}\n",
    "option.tensor_parallel_degree=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a67a357-7358-4ed3-977b-874590542e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_Salesforce_xgen_7b_8k_base_accelerate/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/requirements.txt\n",
    "torch==2.0.1\n",
    "einops==0.5.0\n",
    "tiktoken\n",
    "transformers==4.30.2\n",
    "accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e733f6-91a3-4806-829e-8483b9f92d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m=\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     2\t\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33mSalesforce/xgen-7b-8k-base\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     3\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{directory_name}/serving.properties\").open().read())\n",
    "Path(f\"{directory_name}/serving.properties\").open(\"w\").write(\n",
    "    template.render(hf_model_id=hf_model_id)\n",
    ")\n",
    "!pygmentize {directory_name}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a2900-ef24-4426-bc5c-fd85c5ad269a",
   "metadata": {},
   "source": [
    "### 2. Create a model.py with custom inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8fbf7-608d-4223-b943-83dcd87ef335",
   "metadata": {},
   "source": [
    "With SageMaker, you have the flexibility to bring your own script for inference. In this case, we need to create a model.py file with the necessary code for the Salesforce Xgen-7b-8k-base model.\n",
    "\n",
    "I've provided two scripts below, and both of them will work. However, I recommend using the second one (uncommented) as it produces slightly faster responses. This is because it utilizes the generate() API instead of the pipeline() API.\n",
    "\n",
    "If you'd like more information on the difference between the pipeline and generate APIs, you can check out this helpful [Ref](https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203). 📚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1471249-d7b2-4195-9c05-3953341cc121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile ./{directory_name}/model.py\n",
    "# from djl_python import Input, Output\n",
    "# import os\n",
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "# from typing import Any, Dict, Tuple\n",
    "# import warnings\n",
    "\n",
    "# predictor = None\n",
    "# print(\"transformers version\"+ transformers.__version__)\n",
    "\n",
    "\n",
    "# def get_model(properties):\n",
    "#     model_name = properties[\"model_id\"]\n",
    "#     local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "#                                           trust_remote_code=True,\n",
    "#                                          )\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                                  torch_dtype=torch.bfloat16,\n",
    "#                                                  device_map=\"auto\"\n",
    "#                                                 )\n",
    "#     generator = pipeline(\n",
    "#         task=\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    "#     )\n",
    "#     return generator\n",
    "\n",
    "\n",
    "# def handle(inputs: Input) -> None:\n",
    "#     global predictor\n",
    "#     if not predictor:\n",
    "#         predictor = get_model(inputs.get_properties())\n",
    "#     if inputs.is_empty():\n",
    "#         # Model server makes an empty call to warmup the model on startup\n",
    "#         return None\n",
    "#     data = inputs.get_as_json()\n",
    "#     text = data.pop(\"text\", data)\n",
    "#     parameters = data.pop(\"parameters\", None)\n",
    "#     outputs = predictor(text, **parameters)\n",
    "#     result = {\"generated_text\": outputs[0]['generated_text']}\n",
    "#     return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6591940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_Salesforce_xgen_7b_8k_base_accelerate/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Any, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "predictor = None\n",
    "print(\"transformers version\"+ transformers.__version__)\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True,\n",
    "                                         )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 device_map=\"auto\"\n",
    "                                                )\n",
    "    predictor = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "    return predictor \n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    model, tokenizer = predictor[\"model\"], predictor[\"tokenizer\"]\n",
    "    data = inputs.get_as_json()\n",
    "    text = data.pop(\"text\", data)\n",
    "    params = data.pop(\"parameters\", None)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        sample = model.generate(**inputs, **params)\n",
    "    result = {\"generated_text\": tokenizer.decode(sample[0])}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f3ac7-dd27-4e4b-b5a5-d71cb8bc0733",
   "metadata": {},
   "source": [
    "### 3. Create the Tarball and then upload to S3 location\n",
    "Now, let's package our artifacts as *.tar.gz files, which we'll upload to S3. These files will be used by SageMaker for deployment. 📦💨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae6f030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./requirements.txt\n",
      "./serving.properties\n",
      "./model.py\n",
      "S3 Code or Model tar for accelerate uploaded to --- > s3://sagemaker-eu-west-1-069230569860/hf-large-model/Salesforce-xgen-7b-8k-base/accelerate/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf {directory_name}/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C {directory_name} .\n",
    "s3_code_artifact_accelerate = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix_accelerate)\n",
    "print(f\"S3 Code or Model tar for accelerate uploaded to --- > {s3_code_artifact_accelerate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbc98f-2991-4fed-ae7b-5d801b15ac18",
   "metadata": {},
   "source": [
    "### 4. Define a serving container, SageMaker Model and SageMaker endpoint\n",
    "\n",
    "Now, we can move on to creating a SageMaker endpoint to serve our model. 🚀\n",
    "\n",
    "#### Define the serving container\n",
    "In this step, we'll specify the container to be used for the model during inference. For optimal performance, we'll be using SageMaker's Large Model Inference (LMI) container with Accelerate. ⚡️🧪 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae731378-f75d-41bc-ba67-cabdf4ca6cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.eu-west-1.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118\"\n",
    ")\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648de8f-351d-4f1b-96a7-899b413ff8f4",
   "metadata": {},
   "source": [
    "#### Create SageMaker model, endpoint configuration and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327a864e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807\n"
     ]
    }
   ],
   "source": [
    "model_name_acc = name_from_base(model_id)\n",
    "print(model_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178c74e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model: arn:aws:sagemaker:eu-west-1:069230569860:model/salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_acc,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact_accelerate},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37baaaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building EndpointConfig and Endpoint for: Salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name_acc\n",
    "print(f\"Building EndpointConfig and Endpoint for: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a6a0a9c-d2be-4548-a78c-4ad2cbab9087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:eu-west-1:069230569860:endpoint-config/salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807-config',\n",
       " 'ResponseMetadata': {'RequestId': 'e48feb47-1c25-4a43-b7e4-b1c602c22483',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e48feb47-1c25-4a43-b7e4-b1c602c22483',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '138',\n",
       "   'date': 'Thu, 29 Jun 2023 14:03:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            # \"VolumeSizeInGB\": 512\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34c6563-75bc-4441-b14a-0ea3694ce61f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:eu-west-1:069230569860:endpoint/salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e348ac-b9b4-4998-b9bd-51d0d07f51c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:eu-west-1:069230569860:endpoint/salesforce-xgen-7b-8k-base-2023-06-29-14-03-10-807-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6750d-d680-42f3-b879-005c4c006add",
   "metadata": {},
   "source": [
    "### Let's use the endpoint & run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d9b6bd-602b-4c28-9c06-5297a1af4017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.8 s ± 86.7 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 5\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"text\": \"The population of Greece is\",\n",
    "                     \"parameters\": {\n",
    "                          \"max_new_tokens\": 500, #the higher the longer the response time\n",
    "                          \"temperature\": 0.1,\n",
    "                          \"top_p\": 0.75,\n",
    "                          \"top_k\": 40,\n",
    "                          \"repetition_penalty\": 1.9,\n",
    "                          \"do_sample\": True,\n",
    "                          \"num_return_sequences\": 1,\n",
    "                          #\"return_full_text\":False, # avoid returning pr\n",
    "                          \"best_of\": None, \n",
    "                          \"truncate\": None,\n",
    "                     }}\n",
    "                     ),\n",
    "    ContentType=\"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bfbfb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10.8 million people (2016). The country has a very high life expectancy, with the average Greek living to be 79 years old and women live on an even longer time – 83 year-old!\n",
      "Greece’s economy relies heavily upon tourism as it accounts for about 20% GDP in total revenue from all sources including exports which are also significant at around 15%. Tourism makes up over 30 percent or more than $10 billion dollars annually according some estimates while others say that figure could reach closer towards 40%, but either way this number will continue growing due mainly because there aren't enough hotels available right now so demand keeps increasing every day without any sign off slowing down anytime soon especially since they have just opened their first ever casino last month called “Konstantinos Casino Resort & Spa\" located near Athens International Airport where many international travelers pass through daily en route home after visiting other countries like Italy etc., thus creating yet another opportunity not only within Europe itself -but worldwide too-, making sure everyone knows exactly what kind quality service you can expect when coming here whether looking forward toward vacationing purposes alone;or if planning business trips involving meetings/conferences held inside these facilities themselves?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The population of Greece is\"\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"text\": prompt,\n",
    "                     \"parameters\": {\n",
    "                          \"max_new_tokens\": 500, #the higher the longer the response time\n",
    "                          \"temperature\": 0.01,\n",
    "                          \"top_p\": 0.85,\n",
    "                          \"top_k\": 40,\n",
    "                          \"repetition_penalty\": 1.9,\n",
    "                          \"do_sample\": True,\n",
    "                          \"num_return_sequences\": 1,\n",
    "                          #\"return_full_text\":False, # avoid returning pr\n",
    "                          \"best_of\": None, \n",
    "                          \"truncate\": None,\n",
    "                     }}\n",
    "                     ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "def process_generated_text(text, stopwords, prompt=None):\n",
    "    if prompt:\n",
    "        text = text[len(prompt):]\n",
    "    \n",
    "    for word in stopwords:\n",
    "        position = text.find(word)\n",
    "        if position != -1:\n",
    "            text = text[:position]\n",
    "    return text\n",
    "    \n",
    "\n",
    "r = response_model[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "# Load the JSON string as a dictionary\n",
    "data_dict = json.loads(r)\n",
    "\n",
    "# Access the dictionary elements\n",
    "generated_text = data_dict['generated_text']\n",
    "\n",
    "# Print the generated text\n",
    "print(process_generated_text(generated_text, ['<|endoftext|>'], prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59f5b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The population of Greece is 10.8 million people (2016). The country has a very high life expectancy, with the average Greek living to be 79 years old and women live on an even longer time – 83 year-old!\n",
      "Greece’s economy relies heavily upon tourism as it accounts for about 20% GDP in total revenue from all sources including exports which are also significant at around 15%. Tourism makes up over 30 percent or more than $10 billion dollars annually according some estimates while others say that figure could reach closer towards 40%, but either way this number will continue growing due mainly because there aren't enough hotels available right now so demand keeps increasing every day without any sign off slowing down anytime soon especially since they have just opened their first ever casino last month called “Konstantinos Casino Resort & Spa\" located near Athens International Airport where many international travelers pass through daily en route home after visiting other countries like Italy etc., thus creating yet another opportunity not only within Europe itself -but worldwide too-, making sure everyone knows exactly what kind quality service you can expect when coming here whether looking forward toward vacationing purposes alone;or if planning business trips involving meetings/conferences held inside these facilities themselves?<|endoftext|>Home » Newsroom Archive | Page 2\t»\n",
      "News archive: page 1 /2\n",
      "New research shows how we use our brains differently during sleep compared to wakefulness — ScienceDaily January 17th 2021 https://www.sciencedaily.com… via @ScienceMagazine #sleepresearch … See MoreSee Less\n",
      "How do your brain cells know who goes out late into town each night versus those staying close by sleeping soundly under one roof?, asks Dr David Eagleman (@davideagle) , Professor Of Neurobiology And Psychology At Stanford University In California USA . Read his latest paper published today online ahead o f print publication date : \"Sleep Is A Brain State That Encodes Information About Who Goes Out Late Into Town Each Night Versus Those Staying Close By Sleeping Soundlessly Under One Roof.\" ...Read Full Story Here ➡️https:/t….eepurl…..io/?utm_source=newsletter&amp#\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeec2f9-51f5-406b-b8cb-615ac67a709c",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3744ca9-d6cf-4aa4-8693-4fd43a71e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '287929f4-f5de-4dd7-9516-4eb9a59200e3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '287929f4-f5de-4dd7-9516-4eb9a59200e3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 29 Jun 2023 14:21:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c05cfff2-a56d-4fe4-8915-74e44b68715e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '514786ec-624f-46cc-b2cc-78d8af29508a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '514786ec-624f-46cc-b2cc-78d8af29508a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 29 Jun 2023 14:21:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the model and the endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06e946-c156-4d33-82bf-ec94db6cda73",
   "metadata": {},
   "source": [
    "# Delete all endpoints, endpoint configurations & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7daaff3a-15f8-4198-bedc-12dfca1b9736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: Salesforce-xgen-7b-8k-base-2023-06-29-01-22-40-473\n",
      "Deleted\n",
      "Deleting model: Salesforce-xgen-7b-8k-base-2023-06-29-00-07-15-303\n",
      "Deleted\n",
      "Deleting model: Salesforce-xgen-7b-8k-base-2023-06-29-00-05-44-523\n",
      "Deleted\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-10-46-22-359-endpoint\n",
      "An error occurred: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint \"arn:aws:sagemaker:eu-west-1:069230569860:endpoint/salesforce-xgen-7b-8k-base-2023-06-29-10-46-22-359-endpoint\".\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-01-36-44-565-endpoint\n",
      "Deleted\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-01-30-00-053-endpoint\n",
      "Deleted\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-01-22-40-473-endpoint\n",
      "Deleted\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-00-07-15-303-endpoint\n",
      "Deleted\n",
      "Deleting endpoint: Salesforce-xgen-7b-8k-base-2023-06-29-00-05-44-523-endpoint\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-11-14-38-231-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-11-13-22-525-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-10-46-22-359-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-10-20-40-067-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-10-19-39-923-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-01-39-37-338-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-01-39-13-717-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-01-38-28-665-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-01-36-44-565-config\n",
      "Deleted\n",
      "Deleting endpoint_config: Salesforce-xgen-7b-8k-base-2023-06-29-01-30-00-053-config\n",
      "Deleted\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def delete_resources(resource_type):\n",
    "    client = boto3.client('sagemaker')\n",
    "    list_method = getattr(client, f\"list_{resource_type}s\")\n",
    "    delete_method = getattr(client, f\"delete_{resource_type}\")\n",
    "    resource_type_name = resource_type.replace('_', ' ').title().replace(' ', '')\n",
    "    resources = list_method()[f\"{resource_type_name}s\"]\n",
    "    for resource in resources:\n",
    "        resource_name = resource[f\"{resource_type_name}Name\"]\n",
    "        print(f\"Deleting {resource_type}: {resource_name}\")\n",
    "        try:\n",
    "            delete_method(**{f\"{resource_type_name}Name\": resource_name})\n",
    "            print(\"Deleted\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "\n",
    "def main():\n",
    "    resource_types = ['model', 'endpoint', 'endpoint_config']  # Add more resource types if needed\n",
    "\n",
    "    for resource_type in resource_types:\n",
    "        delete_resources(resource_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2865f-eac6-43eb-96e3-44af99ed0c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
