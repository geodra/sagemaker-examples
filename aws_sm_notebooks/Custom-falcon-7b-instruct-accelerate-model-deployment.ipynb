{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fce004-0da0-4337-becb-a7ff859502ca",
   "metadata": {},
   "source": [
    "#  Serve Custom tiiuae/falcon-7b-instruct model with Amazon SageMaker Hosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b832e6-641a-443c-b533-2396e19b01bc",
   "metadata": {},
   "source": [
    "üëã Hey there! üåü \n",
    "\n",
    "Let's take a walk-through together on how to deploy and perform inference on a custom **tiiuae/falcon-7b-instruct** model using the **Large Model Inference (LMI)** container provided by AWS with the help of **DJL Serving**. üòÑ\n",
    "\n",
    "Since the **tiiuae/falcon-7b-instruct** is a relatively small language model (LLM) that can be easily accommodated on a single GPU, we'll make use of the `ml.g5.2xlarge` instance, which comes with **1** GPU. üñ•Ô∏è\n",
    "\n",
    "**Note:** Use an environment that has PyTorch torch==2.0.1 or install by running the following command on a cell -> `pip install torch==2.0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad5b38-cb9c-4df9-ada8-84d0bed0c91a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbea2a-b6c8-4106-ae69-17c877a00f88",
   "metadata": {},
   "source": [
    "To get started, you'll need to install the necessary dependencies for packaging your model and running inferences on Amazon SageMaker. Don't worry, it's a simple process! Just make sure to update SageMaker and boto3 too. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd50d8cb-1e06-480a-afb9-a5cc770e3c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade  --quiet\n",
    "!pip install transformers einops==0.5.0 tiktoken accelerate --q\n",
    "# Uncomment if you are building your own env otherwise select one environmanet with torch==2.0.1 pre-installed\n",
    "#!pip install torch==2.0.1 --q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80a142-7f21-48ca-9a44-1462cd82004c",
   "metadata": {},
   "source": [
    "## Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b43806-e267-4259-8dd8-617d4feb839d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import boto3\n",
    "import jinja2\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2cb0e7-e646-4a8b-b9b5-f25c08ec6b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "hf_model_id = 'tiiuae/falcon-7b-instruct'\n",
    "model_id = hf_model_id.replace('/','-')\n",
    "s3_code_prefix_accelerate = f\"hf-large-model/{model_id}/accelerate\"  # folder within bucket where code artifact will go\n",
    "s3_model = f\"hf-large-model/{model_id}/model\"\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed77f2c",
   "metadata": {},
   "source": [
    "First, let's load the tiiuae/falcon-7b-instruct model along with its tokenizer and save it to S3. üöÄ The great thing is that you can also train it on your own custom dataset and store the trained model on S3.\n",
    "\n",
    "**Note:** While you may not plan to fine-tune it, using the container to directly download the model from the HuggingFace Hub can result in **long download times.** ‚è≥ It's highly recommended to download the model from the HuggingFace Hub to your local host instead. Once downloaded, simply upload the model to an S3 Bucket. üì• If you have any questions or need further assistance, feel free to ask! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036b2629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd6f7a0ba0401fa4aecf9f15e5acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                      trust_remote_code=True,\n",
    "                                      cache_dir=os.path.join(os.environ['PWD'],'hf/tokenizer_cache/')\n",
    "                                     )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             cache_dir=os.path.join(os.environ['PWD'],'hf/model_cache/')\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15db91d",
   "metadata": {},
   "source": [
    "Save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training code to finetune model on a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792552d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ec2-user/SageMaker/sm_model/tokenizer_config.json',\n",
       " '/home/ec2-user/SageMaker/sm_model/special_tokens_map.json',\n",
       " '/home/ec2-user/SageMaker/sm_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_path = os.path.join(os.environ['PWD'],'sm_model/')\n",
    "model.save_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679777c",
   "metadata": {},
   "source": [
    "Upload to S3. It will be used to quickly load the model during deployment and avoiding it downloading from huggingface hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b3fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /home/ec2-user/SageMaker/sm_model/generation_config.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/generation_config.json\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/pytorch_model-00001-of-00002.bin to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/pytorch_model-00001-of-00002.bin\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/tokenizer_config.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/tokenizer_config.json\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/pytorch_model.bin.index.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/pytorch_model.bin.index.json\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/config.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/config.json\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/pytorch_model-00002-of-00002.bin to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/pytorch_model-00002-of-00002.bin\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/special_tokens_map.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/special_tokens_map.json\n",
      "Uploaded /home/ec2-user/SageMaker/sm_model/tokenizer.json to s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "def upload_directory_to_s3(local_directory, s3_bucket, s3_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            s3_key = os.path.join(s3_prefix, os.path.relpath(local_path, local_directory))\n",
    "            s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "            print(f\"Uploaded {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "# Example usage\n",
    "upload_directory_to_s3(local_model_path, model_bucket, s3_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bdc38-8552-4d2a-b1fe-aeacc229b85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Create SageMaker compatible model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f3f1f-d82e-403c-b966-a1a495a26d5d",
   "metadata": {},
   "source": [
    "To get our model ready for deployment on a SageMaker Endpoint, we need to prepare a few things for both SageMaker and our container. No worries, it's a straightforward process! We'll use a local folder to store these files, including some important ones like serving.properties (which defines parameters for the LMI container) and requirements.txt (to specify the dependencies we need to install). üìÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8d9604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory_name = f\"code_{model_id.replace('-','_')}_accelerate\"\n",
    "os.makedirs(directory_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c1b6f-213b-4540-b65c-c2c2e029e283",
   "metadata": {},
   "source": [
    "In the serving.properties file, you'll need to define the engine to use and the model you want to host. Pay attention to the tensor_parallel_degree parameter, as it's essential in this scenario. If a single GPU doesn't have enough memory to handle the entire model, you can use tensor parallelism >1 to divide the model into multiple parts.\n",
    "\n",
    "For your deployment, we'll be using a 'ml.g5.2xlarge' instance, which comes with 1 GPU and is sufficient for loading our model. Just make sure not to specify a value larger than what the instance provides, or your deployment might encounter issues. ‚ùåüôÖ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Finally, we need to specify the S3 link where the model can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212382fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_tiiuae_falcon_7b_instruct_accelerate/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url={{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a67a357-7358-4ed3-977b-874590542e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_tiiuae_falcon_7b_instruct_accelerate/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/requirements.txt\n",
    "torch==2.0.1\n",
    "einops==0.5.0\n",
    "tiktoken\n",
    "transformers==4.30.2\n",
    "accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e733f6-91a3-4806-829e-8483b9f92d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m=\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     2\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     3\t\u001b[36moption.s3url\u001b[39;49;00m=\u001b[33ms3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{directory_name}/serving.properties\").open().read())\n",
    "Path(f\"{directory_name}/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=f\"s3://{model_bucket}/{s3_model}\")\n",
    ")\n",
    "!pygmentize {directory_name}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a2900-ef24-4426-bc5c-fd85c5ad269a",
   "metadata": {},
   "source": [
    "### 2. Create a model.py with custom inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8fbf7-608d-4223-b943-83dcd87ef335",
   "metadata": {},
   "source": [
    "With SageMaker, you have the flexibility to bring your own script for inference. In this case, we need to create a model.py file with the necessary code for the Salesforce Xgen-7b-8k-base model.\n",
    "\n",
    "I've provided two scripts below, and both of them will work. However, I recommend using the second one (uncommented) as it produces slightly faster responses. This is because it utilizes the generate() API instead of the pipeline() API.\n",
    "\n",
    "If you'd like more information on the difference between the pipeline and generate APIs, you can check out this helpful [Ref](https://discuss.huggingface.co/t/pipeline-vs-model-generate/26203). üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6591940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code_tiiuae_falcon_7b_instruct_accelerate/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{directory_name}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Any, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "predictor = None\n",
    "print(\"transformers version\"+ transformers.__version__)\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    \"\"\"\n",
    "    In case you want to have a look at the env variables set by SageMaker or the paths inside the container\n",
    "    try:\n",
    "        print('properties')\n",
    "        for key, value in properties.items():\n",
    "            print(key, value)\n",
    "        print(20*'--')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('os.environ')\n",
    "        for key, value in os.environ.items():\n",
    "            print(key, value)\n",
    "        print(20*'--')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        root_dir = '/opt'\n",
    "        print('root_dir')\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(file_path)\n",
    "    except:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = properties[\"model_id\"]\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True,\n",
    "                                         )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True,\n",
    "                                                )\n",
    "    predictor = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "    return predictor \n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    model, tokenizer = predictor[\"model\"], predictor[\"tokenizer\"]\n",
    "    data = inputs.get_as_json()\n",
    "    text = data.pop(\"text\", data)\n",
    "    params = data.pop(\"parameters\", None)\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        sample = model.generate(input_ids=encoding.input_ids,\n",
    "                                attention_mask=encoding.attention_mask,\n",
    "                                **params)\n",
    "    result = {\"generated_text\": tokenizer.decode(sample[0])}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f3ac7-dd27-4e4b-b5a5-d71cb8bc0733",
   "metadata": {},
   "source": [
    "### 3. Create the Tarball and then upload to S3 location\n",
    "Now, let's package our artifacts as *.tar.gz files, which we'll upload to S3. These files will be used by SageMaker for deployment. üì¶üí®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae6f030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./requirements.txt\n",
      "./serving.properties\n",
      "./model.py\n",
      "S3 Code or Model tar for accelerate uploaded to --- > s3://sagemaker-eu-west-1-069230569860/hf-large-model/tiiuae-falcon-7b-instruct/accelerate/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf {directory_name}/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C {directory_name} .\n",
    "s3_code_artifact_accelerate = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix_accelerate)\n",
    "print(f\"S3 Code or Model tar for accelerate uploaded to --- > {s3_code_artifact_accelerate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbc98f-2991-4fed-ae7b-5d801b15ac18",
   "metadata": {},
   "source": [
    "### 4. Define a serving container, SageMaker Model and SageMaker endpoint\n",
    "\n",
    "Now, we can move on to creating a SageMaker endpoint to serve our model. üöÄ\n",
    "\n",
    "#### Define the serving container\n",
    "In this step, we'll specify the container to be used for the model during inference. For optimal performance, we'll be using SageMaker's Large Model Inference (LMI) container with Accelerate. ‚ö°Ô∏èüß™ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae731378-f75d-41bc-ba67-cabdf4ca6cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.eu-west-1.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.8.3-cu118\"\n",
    ")\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648de8f-351d-4f1b-96a7-899b413ff8f4",
   "metadata": {},
   "source": [
    "#### Create SageMaker model, endpoint configuration and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327a864e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513\n"
     ]
    }
   ],
   "source": [
    "model_name_acc = name_from_base(model_id)\n",
    "print(model_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178c74e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model: arn:aws:sagemaker:eu-west-1:069230569860:model/tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_acc,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact_accelerate},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f37baaaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building EndpointConfig and Endpoint for: tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name_acc\n",
    "print(f\"Building EndpointConfig and Endpoint for: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a6a0a9c-d2be-4548-a78c-4ad2cbab9087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:eu-west-1:069230569860:endpoint-config/tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513-config',\n",
       " 'ResponseMetadata': {'RequestId': '20784c10-0f05-4f0c-92ec-a2683c01bf02',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '20784c10-0f05-4f0c-92ec-a2683c01bf02',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '137',\n",
       "   'date': 'Thu, 29 Jun 2023 17:37:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            # \"VolumeSizeInGB\": 512\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f34c6563-75bc-4441-b14a-0ea3694ce61f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:eu-west-1:069230569860:endpoint/tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e348ac-b9b4-4998-b9bd-51d0d07f51c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:eu-west-1:069230569860:endpoint/tiiuae-falcon-7b-instruct-2023-06-29-17-37-27-513-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6750d-d680-42f3-b879-005c4c006add",
   "metadata": {},
   "source": [
    "### Let's use the endpoint & run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d9b6bd-602b-4c28-9c06-5297a1af4017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.89 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "15 s ¬± 6.74 s per loop (mean ¬± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 5\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"text\": \"The population of Greece is\",\n",
    "                     \"parameters\": {\n",
    "                          \"max_new_tokens\": 500, #the higher the longer the response time\n",
    "                          \"temperature\": 0.1,\n",
    "                          \"top_p\": 0.85,\n",
    "                          \"top_k\": 40,\n",
    "                          \"repetition_penalty\": 1.9,\n",
    "                          \"do_sample\": True,\n",
    "                          \"num_return_sequences\": 1,\n",
    "                          #\"return_full_text\":False, # avoid returning pr\n",
    "                          \"best_of\": None, \n",
    "                          \"truncate\": None,\n",
    "                     }}\n",
    "                     ),\n",
    "    ContentType=\"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad79dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The population of Greece is 10.7 million people.\n",
      "Greece has a population density of 268 people per square kilometer.\n",
      "Greece's population growth rate is 0.2%.\n",
      "Greece's population is predominantly urban, with 82.4% of the population living in cities.\n",
      "Greece's population distribution is relatively evenly spread out across its various regions.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The population of Greece is\"\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"text\": prompt,\n",
    "                     \"parameters\": {\n",
    "                          \"max_new_tokens\": 500, #the higher the longer the response time\n",
    "                          \"temperature\": 0.1,\n",
    "                          \"top_p\": 0.85,\n",
    "                          \"top_k\": 40,\n",
    "                          \"repetition_penalty\": 1.9,\n",
    "                          \"do_sample\": True,\n",
    "                          \"num_return_sequences\": 1,\n",
    "                          #\"return_full_text\":False, # avoid returning pr\n",
    "                          \"best_of\": None, \n",
    "                          \"truncate\": None,\n",
    "                     }}\n",
    "                     ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "r = response_model[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "# Load the JSON string as a dictionary\n",
    "data_dict = json.loads(r)\n",
    "\n",
    "# Access the dictionary elements\n",
    "generated_text = data_dict['generated_text']\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b4e4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10.7 million people.\n",
      "Greece has a population density of 268 people per square kilometer.\n",
      "Greece's population growth rate is 0.2%.\n",
      "Greece's population is predominantly urban, with 82.4% of the population living in cities.\n",
      "Greece's population distribution is relatively evenly spread out across its various regions.\n"
     ]
    }
   ],
   "source": [
    "def process_generated_text(text, stopwords, prompt=None):\n",
    "    if prompt:\n",
    "        text = text[len(prompt):]\n",
    "    \n",
    "    for word in stopwords:\n",
    "        position = text.find(word)\n",
    "        if position != -1:\n",
    "            text = text[:position]\n",
    "    return text\n",
    "\n",
    "# Print the generated text\n",
    "print(process_generated_text(generated_text, ['<|endoftext|>'], prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeec2f9-51f5-406b-b8cb-615ac67a709c",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3744ca9-d6cf-4aa4-8693-4fd43a71e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'b48dc2f0-d001-4e6f-9828-b819df84607a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b48dc2f0-d001-4e6f-9828-b819df84607a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 29 Jun 2023 17:48:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c05cfff2-a56d-4fe4-8915-74e44b68715e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '34ead684-7907-41ee-abc4-f2143cd12058',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '34ead684-7907-41ee-abc4-f2143cd12058',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 29 Jun 2023 17:48:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the model and the endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06e946-c156-4d33-82bf-ec94db6cda73",
   "metadata": {},
   "source": [
    "# Delete all endpoints, endpoint configurations & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7daaff3a-15f8-4198-bedc-12dfca1b9736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: tiiuae-falcon-7b-instruct-2023-06-29-17-30-23-944\n",
      "Deleted\n",
      "Deleting endpoint_config: tiiuae-falcon-7b-instruct-2023-06-29-17-30-23-944-config\n",
      "Deleted\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def delete_resources(resource_type):\n",
    "    client = boto3.client('sagemaker')\n",
    "    list_method = getattr(client, f\"list_{resource_type}s\")\n",
    "    delete_method = getattr(client, f\"delete_{resource_type}\")\n",
    "    resource_type_name = resource_type.replace('_', ' ').title().replace(' ', '')\n",
    "    resources = list_method()[f\"{resource_type_name}s\"]\n",
    "    for resource in resources:\n",
    "        resource_name = resource[f\"{resource_type_name}Name\"]\n",
    "        print(f\"Deleting {resource_type}: {resource_name}\")\n",
    "        try:\n",
    "            delete_method(**{f\"{resource_type_name}Name\": resource_name})\n",
    "            print(\"Deleted\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "\n",
    "def main():\n",
    "    resource_types = ['model', 'endpoint', 'endpoint_config']  # Add more resource types if needed\n",
    "\n",
    "    for resource_type in resource_types:\n",
    "        delete_resources(resource_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
